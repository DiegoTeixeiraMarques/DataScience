{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "UNIVERSIDADE DE FORTALEZA\n",
    "\n",
    "P√ìS GRADUA√á√ÉO - MBA EM CI√äNCIA DE DADOS\n",
    "\n",
    "Prof. PhD Ernerson A. Oliveira\n",
    "\n",
    "Aluno: Diego Teixeira\n",
    "\n",
    "Matricula: 1924526\n",
    "\n",
    "**TRABALHO DE AN√ÅLISE DOS TWITTERS DO PRES. JAIR MESSIAS BOLSONARO E DO EX-PRES. LU√çS IN√ÅCIO LULA DA SILVA**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**1. INTRODU√á√ÉO**\n",
    "\n",
    "O trabalho busca analisar os conte√∫dos postados nos Twitters do ent√£o Presidente Jair Messias Bolsonaro e do ex-Presidente Lu√≠s In√°cio Lula da Silva, que buscam se posicionar junto aos seus apoiadores ou n√£o, diante do cen√°rio pol√≠tico que o Brasil tem vivenciado nos √∫ltimos anos.\n",
    "\n",
    "√â importante destacar que a an√°lise desenvolvida cont√©m total isen√ß√£o ideol√≥gica e partid√°ria, contribuindo assim para um resultado sem qualquer vi√©s pol√≠tico. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**2. METODOLOGIA**\n",
    "\n",
    "Os downloads dos tweets de cada conta encontram-se em formato json e foram no total de 6.794 do Presidente Jair Bolsonaro, compreendido entre o per√≠odo de 01/04/2010 a 08/01/2020, e de 14.961 do ex-Presidente Lula, de 02/09/2014 a 08/01/2020.\n",
    "\n",
    "Para analisar os tweets dos referidos pol√≠ticos, foram utilizadas ferramentas de minera√ß√£o de texto em linguagem de programa√ß√£o. Foi adotada o Python com os conjuntos de bibliotecas do Pandas e do Natural Language Toolkit (NLTK) onde inicialmente buscou-se entender os dados, e em seguida realizar a limpeza dos arquivos (Higieniza√ß√£o), retirando pontua√ß√£o, caracteres especiais, palavras esparsas, n√∫meros e palavras sem valor anal√≠tico ou stopwords, mantendo-se apenas o conte√∫do dos textos.\n",
    "\n",
    "A partir desta etapa foi poss√≠vel identificar e definir insights para o desenvolvimento do estudo apresentado abaixo, utilizando-se do Matplotlib e do WordCloud."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3. CONHECENDO OS DADOS**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.1. Importando o M√≥dulo Pandas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import nltk\n",
    "#nltk.download()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2. Dados em DataFrame**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.1. Verificando as 5 linhas primeiras do DataFrame de Jair Bolsonaro** "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>full_text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>source</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>lang</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>quoted_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2010-04-01 02:59:50+00:00</td>\n",
       "      <td>11402700435</td>\n",
       "      <td>11402700435</td>\n",
       "      <td>EM \"DITADURA\" SEM PARED√ÉO, AT√â CHICO ALENCAR √â...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 137]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"https://about.twitter.com/products/tw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>15</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2019-01-10 11:02:23+00:00</td>\n",
       "      <td>1083318129135112192</td>\n",
       "      <td>1083318129135112192</td>\n",
       "      <td>Bom dia! üáßüá∑ #tbt com o amigo \"Canguru\", que j√°...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 90]</td>\n",
       "      <td>{'hashtags': [{'text': 'tbt', 'indices': [12, ...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>31618</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>{'media': [{'id': 1083318121841262592, 'id_str...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2019-07-19 02:04:16+00:00</td>\n",
       "      <td>1152036400138579968</td>\n",
       "      <td>1152036400138579968</td>\n",
       "      <td>- Para descontrair. Proibido queimar ovo. (Kkk...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 48]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/android\" ...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>46637</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>{'media': [{'id': 1152036204247953408, 'id_str...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2018-10-09 02:52:56+00:00</td>\n",
       "      <td>1049492883328380928</td>\n",
       "      <td>1049492883328380928</td>\n",
       "      <td>Trecho de entrevista ao vivo para o Jornal Nac...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 65]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>56322</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>{'media': [{'id': 1049489244387983360, 'id_str...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2017-08-23 15:12:48+00:00</td>\n",
       "      <td>900375277557215232</td>\n",
       "      <td>900375277557215232</td>\n",
       "      <td>Querem criar o fund√£o bilion√°rio na Reforma Po...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 138]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>7147</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                   id               id_str  \\\n",
       "0 2010-04-01 02:59:50+00:00          11402700435          11402700435   \n",
       "1 2019-01-10 11:02:23+00:00  1083318129135112192  1083318129135112192   \n",
       "2 2019-07-19 02:04:16+00:00  1152036400138579968  1152036400138579968   \n",
       "3 2018-10-09 02:52:56+00:00  1049492883328380928  1049492883328380928   \n",
       "4 2017-08-23 15:12:48+00:00   900375277557215232   900375277557215232   \n",
       "\n",
       "                                           full_text  truncated  \\\n",
       "0  EM \"DITADURA\" SEM PARED√ÉO, AT√â CHICO ALENCAR √â...      False   \n",
       "1  Bom dia! üáßüá∑ #tbt com o amigo \"Canguru\", que j√°...      False   \n",
       "2  - Para descontrair. Proibido queimar ovo. (Kkk...      False   \n",
       "3  Trecho de entrevista ao vivo para o Jornal Nac...      False   \n",
       "4  Querem criar o fund√£o bilion√°rio na Reforma Po...      False   \n",
       "\n",
       "  display_text_range                                           entities  \\\n",
       "0           [0, 137]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "1            [0, 90]  {'hashtags': [{'text': 'tbt', 'indices': [12, ...   \n",
       "2            [0, 48]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "3            [0, 65]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "4           [0, 138]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                                              source  in_reply_to_status_id  \\\n",
       "0  <a href=\"https://about.twitter.com/products/tw...                    NaN   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...                    NaN   \n",
       "2  <a href=\"http://twitter.com/download/android\" ...                    NaN   \n",
       "3  <a href=\"http://twitter.com/download/iphone\" r...                    NaN   \n",
       "4  <a href=\"http://twitter.com/download/iphone\" r...                    NaN   \n",
       "\n",
       "   in_reply_to_status_id_str  ...  favorite_count  favorited retweeted lang  \\\n",
       "0                        NaN  ...              15      False     False   pt   \n",
       "1                        NaN  ...           31618      False     False   pt   \n",
       "2                        NaN  ...           46637      False     False   pt   \n",
       "3                        NaN  ...           56322      False     False   pt   \n",
       "4                        NaN  ...            7147      False     False   pt   \n",
       "\n",
       "                                   extended_entities  possibly_sensitive  \\\n",
       "0                                                NaN                 NaN   \n",
       "1  {'media': [{'id': 1083318121841262592, 'id_str...                 0.0   \n",
       "2  {'media': [{'id': 1152036204247953408, 'id_str...                 0.0   \n",
       "3  {'media': [{'id': 1049489244387983360, 'id_str...                 0.0   \n",
       "4                                                NaN                 NaN   \n",
       "\n",
       "  quoted_status_id  quoted_status_id_str  quoted_status_permalink  \\\n",
       "0              NaN                   NaN                      NaN   \n",
       "1              NaN                   NaN                      NaN   \n",
       "2              NaN                   NaN                      NaN   \n",
       "3              NaN                   NaN                      NaN   \n",
       "4              NaN                   NaN                      NaN   \n",
       "\n",
       "   quoted_status  \n",
       "0            NaN  \n",
       "1            NaN  \n",
       "2            NaN  \n",
       "3            NaN  \n",
       "4            NaN  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jb = pd.read_json('jairbolsonaro.json')\n",
    "df_jb.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.2.2. Verificando as 5 linhas primeiras do DataFrame de Lula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>created_at</th>\n",
       "      <th>id</th>\n",
       "      <th>id_str</th>\n",
       "      <th>full_text</th>\n",
       "      <th>truncated</th>\n",
       "      <th>display_text_range</th>\n",
       "      <th>entities</th>\n",
       "      <th>source</th>\n",
       "      <th>in_reply_to_status_id</th>\n",
       "      <th>in_reply_to_status_id_str</th>\n",
       "      <th>...</th>\n",
       "      <th>favorite_count</th>\n",
       "      <th>favorited</th>\n",
       "      <th>retweeted</th>\n",
       "      <th>lang</th>\n",
       "      <th>extended_entities</th>\n",
       "      <th>possibly_sensitive</th>\n",
       "      <th>quoted_status_id</th>\n",
       "      <th>quoted_status_id_str</th>\n",
       "      <th>quoted_status_permalink</th>\n",
       "      <th>quoted_status</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>0</td>\n",
       "      <td>2018-08-17 16:23:04+00:00</td>\n",
       "      <td>1030490201460801541</td>\n",
       "      <td>1030490201460801536</td>\n",
       "      <td>\"O presidente Lula vai dar um passeio nessa el...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 212]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>347</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>2014-09-16 01:53:02+00:00</td>\n",
       "      <td>511694215735422976</td>\n",
       "      <td>511694215735422976</td>\n",
       "      <td>@dilmabr afirma que aqueles que querem acabar ...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 139]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com/download/iphone\" r...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>9</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>2017-08-08 23:23:49+00:00</td>\n",
       "      <td>895063025996242944</td>\n",
       "      <td>895063025996242944</td>\n",
       "      <td>O PT deu cidadania para as mulheres fazerem po...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 95]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"https://mobile.twitter.com\" rel=\"nofo...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>203</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>2017-10-18 11:24:23+00:00</td>\n",
       "      <td>920611514071076865</td>\n",
       "      <td>920611514071076864</td>\n",
       "      <td>Continuo sendo o lulinha paz e a amor. A gente...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 108]</td>\n",
       "      <td>{'hashtags': [{'text': 'LulanoRadio', 'indices...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>153</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>2017-04-24 22:24:18+00:00</td>\n",
       "      <td>856634932764037120</td>\n",
       "      <td>856634932764037120</td>\n",
       "      <td>Olhando as √∫ltimas pesquisas, vemos que a √∫nic...</td>\n",
       "      <td>False</td>\n",
       "      <td>[0, 100]</td>\n",
       "      <td>{'hashtags': [], 'symbols': [], 'user_mentions...</td>\n",
       "      <td>&lt;a href=\"http://twitter.com\" rel=\"nofollow\"&gt;Tw...</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>...</td>\n",
       "      <td>168</td>\n",
       "      <td>False</td>\n",
       "      <td>False</td>\n",
       "      <td>pt</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows √ó 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                 created_at                   id               id_str  \\\n",
       "0 2018-08-17 16:23:04+00:00  1030490201460801541  1030490201460801536   \n",
       "1 2014-09-16 01:53:02+00:00   511694215735422976   511694215735422976   \n",
       "2 2017-08-08 23:23:49+00:00   895063025996242944   895063025996242944   \n",
       "3 2017-10-18 11:24:23+00:00   920611514071076865   920611514071076864   \n",
       "4 2017-04-24 22:24:18+00:00   856634932764037120   856634932764037120   \n",
       "\n",
       "                                           full_text  truncated  \\\n",
       "0  \"O presidente Lula vai dar um passeio nessa el...      False   \n",
       "1  @dilmabr afirma que aqueles que querem acabar ...      False   \n",
       "2  O PT deu cidadania para as mulheres fazerem po...      False   \n",
       "3  Continuo sendo o lulinha paz e a amor. A gente...      False   \n",
       "4  Olhando as √∫ltimas pesquisas, vemos que a √∫nic...      False   \n",
       "\n",
       "  display_text_range                                           entities  \\\n",
       "0           [0, 212]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "1           [0, 139]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "2            [0, 95]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "3           [0, 108]  {'hashtags': [{'text': 'LulanoRadio', 'indices...   \n",
       "4           [0, 100]  {'hashtags': [], 'symbols': [], 'user_mentions...   \n",
       "\n",
       "                                              source  in_reply_to_status_id  \\\n",
       "0  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...                    NaN   \n",
       "1  <a href=\"http://twitter.com/download/iphone\" r...                    NaN   \n",
       "2  <a href=\"https://mobile.twitter.com\" rel=\"nofo...                    NaN   \n",
       "3  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...                    NaN   \n",
       "4  <a href=\"http://twitter.com\" rel=\"nofollow\">Tw...                    NaN   \n",
       "\n",
       "   in_reply_to_status_id_str  ...  favorite_count  favorited retweeted lang  \\\n",
       "0                        NaN  ...             347      False     False   pt   \n",
       "1                        NaN  ...               9      False     False   pt   \n",
       "2                        NaN  ...             203      False     False   pt   \n",
       "3                        NaN  ...             153      False     False   pt   \n",
       "4                        NaN  ...             168      False     False   pt   \n",
       "\n",
       "   extended_entities  possibly_sensitive quoted_status_id  \\\n",
       "0                NaN                 NaN              NaN   \n",
       "1                NaN                 NaN              NaN   \n",
       "2                NaN                 NaN              NaN   \n",
       "3                NaN                 NaN              NaN   \n",
       "4                NaN                 NaN              NaN   \n",
       "\n",
       "   quoted_status_id_str  quoted_status_permalink  quoted_status  \n",
       "0                   NaN                      NaN            NaN  \n",
       "1                   NaN                      NaN            NaN  \n",
       "2                   NaN                      NaN            NaN  \n",
       "3                   NaN                      NaN            NaN  \n",
       "4                   NaN                      NaN            NaN  \n",
       "\n",
       "[5 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lu = pd.read_json('LulaOficial.json')\n",
    "df_lu.head(5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3. Conhecendo os DataFrames**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.1. Informa√ß√µes Gerais dos DataFrames**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Qtde. Tweets</th>\n",
       "      <th>Data 1¬∫ Tweet</th>\n",
       "      <th>Data √öltimo Tweet</th>\n",
       "      <th>Qtde.dias no per√≠odo</th>\n",
       "      <th>M√©dia Tweets por dia</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>Bolsonaro</td>\n",
       "      <td>6794</td>\n",
       "      <td>2010-04-01</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>3569</td>\n",
       "      <td>1.90</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>Lula</td>\n",
       "      <td>14961</td>\n",
       "      <td>2014-09-02</td>\n",
       "      <td>2020-01-08</td>\n",
       "      <td>1954</td>\n",
       "      <td>7.66</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "           Qtde. Tweets Data 1¬∫ Tweet Data √öltimo Tweet  Qtde.dias no per√≠odo  \\\n",
       "Bolsonaro          6794    2010-04-01        2020-01-08                  3569   \n",
       "Lula              14961    2014-09-02        2020-01-08                  1954   \n",
       "\n",
       "           M√©dia Tweets por dia  \n",
       "Bolsonaro                  1.90  \n",
       "Lula                       7.66  "
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Atribuindo informa√ß√µes: qtde. de tweets, datas do 1¬∞ e √∫ltimo tweet, qtde. dias entre o 1¬∫ e √∫ltimo tweet e m√©dia tweets/dia\n",
    "qtdTweets_Lula = df_lu.shape[0]\n",
    "qtdTweets_Bols = df_jb.shape[0]\n",
    "dtFirst_Lula = df_lu.created_at.describe()[4].date()\n",
    "dtFirst_Bols = df_jb.created_at.describe()[4].date()\n",
    "dtLast_Lula = df_lu.created_at.describe()[5].date()\n",
    "dtLast_Bols = df_jb.created_at.describe()[5].date()\n",
    "qtdDays_Lula=(dtLast_Lula-dtFirst_Lula).days\n",
    "qtdDays_Bols=(dtLast_Bols-dtFirst_Bols).days\n",
    "mTweetsdia_Lula=round(qtdTweets_Lula/qtdDays_Lula,2)\n",
    "mTweetsdia_Bols=round(qtdTweets_Bols/qtdDays_Bols,2)\n",
    "\n",
    "# Criando um DataFrame com as informa√ß√µes\n",
    "DFs = pd.DataFrame({'Qtde. Tweets': \n",
    "                              {'Bolsonaro': qtdTweets_Bols, \n",
    "                               'Lula': qtdTweets_Lula}, \n",
    "                          'Data 1¬∫ Tweet': \n",
    "                              {'Bolsonaro': dtFirst_Bols, \n",
    "                               'Lula': dtFirst_Lula},\n",
    "                           'Data √öltimo Tweet': \n",
    "                              {'Bolsonaro': dtLast_Bols, \n",
    "                               'Lula': dtLast_Lula},\n",
    "                           'Qtde.dias no per√≠odo':\n",
    "                              {'Bolsonaro':qtdDays_Bols,\n",
    "                               'Lula':qtdDays_Lula},\n",
    "                           'M√©dia Tweets por dia':\n",
    "                              {'Bolsonaro':mTweetsdia_Bols,\n",
    "                               'Lula':mTweetsdia_Lula}\n",
    "                       })\n",
    "DFs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.2. Selecionando as Informa√ß√µes de Jair Bolsonaro para An√°lise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       EM \"DITADURA\" SEM PARED√ÉO, AT√â CHICO ALENCAR √â...\n",
       "1       Bom dia! üáßüá∑ #tbt com o amigo \"Canguru\", que j√°...\n",
       "2       - Para descontrair. Proibido queimar ovo. (Kkk...\n",
       "3       Trecho de entrevista ao vivo para o Jornal Nac...\n",
       "4       Querem criar o fund√£o bilion√°rio na Reforma Po...\n",
       "                              ...                        \n",
       "6789    Menos burocratiza√ß√£o e gastos ao brasileiro: a...\n",
       "6790    A @RevistaEpoca mente DESCARADAMENTE a meu res...\n",
       "6791    Foi realizada hoje a Cerim√¥nia de Posse dos Pr...\n",
       "6792    - O Estatuto do Desarmamento ainda est√° em vig...\n",
       "6793    Discurso d @FlavioBolsonaro sobre maioridade p...\n",
       "Name: full_text, Length: 6794, dtype: object"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_jb.full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**3.3.3. Selecionando as Informa√ß√µes de Lula para An√°lise**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0        \"O presidente Lula vai dar um passeio nessa el...\n",
       "1        @dilmabr afirma que aqueles que querem acabar ...\n",
       "2        O PT deu cidadania para as mulheres fazerem po...\n",
       "3        Continuo sendo o lulinha paz e a amor. A gente...\n",
       "4        Olhando as √∫ltimas pesquisas, vemos que a √∫nic...\n",
       "                               ...                        \n",
       "14956    O prefeito Eduardo Paes e @LindbergFarias acom...\n",
       "14957    Haddad, o candidato de Lula, com Lindberg sena...\n",
       "14958    N√°dia Campe√£o: \"n√£o vamos permitir que essa ca...\n",
       "14959    Os advogados de Lula, Cristiano Zanin e Jos√© R...\n",
       "14960    Seria importante a gente voltar a dezembro de ...\n",
       "Name: full_text, Length: 14961, dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_lu.full_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4. LIMPEZA DOS DADOS DOS DATAFRAMES**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.1. Importando as StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Gerando uma lista do arquivo de StopWords com palavras em min√∫sculo\n",
    "arq_sw = pd.read_csv('stopwords.txt', header = None, names=['palavra'])\n",
    "sw = []\n",
    "for i in range(arq_sw.shape[0]):\n",
    "    sw.append(str(arq_sw.palavra[i]).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.2. Separando as Palavras dos Twitters pelo M√©todo word_tokenize**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Tweets do Bolsonaro\n",
    "palavras_jb = []\n",
    "for i in range(df_jb.shape[0]):\n",
    "    novo = nltk.word_tokenize(df_jb['full_text'][i])\n",
    "    for j in range(len(novo)):\n",
    "        palavras_jb.append(novo[j].lower())\n",
    "\n",
    "# Tweets do Lula\n",
    "palavras_lu = []\n",
    "for i in range(df_lu.shape[0]):\n",
    "    novo = nltk.word_tokenize(df_lu['full_text'][i])\n",
    "    for j in range(len(novo)):\n",
    "        palavras_lu.append(str(novo[j]).lower())"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3. Palavras mais citados nos Tweets do Bolsonaro**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3.1. Removendo as StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_jb_sem_sw = []\n",
    "for palavra in palavras_jb:\n",
    "    if palavra in sw:\n",
    "        #print(palavra)\n",
    "        pass\n",
    "    else:\n",
    "        palavras_jb_sem_sw.append(palavra)\n",
    "#palavras_jb_sem_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.3.2. Quantificando as palavras mais citadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "146          brasil\n",
       "317       bolsonaro\n",
       "56          governo\n",
       "192        obrigado\n",
       "23           abra√ßo\n",
       "300            hoje\n",
       "927              pt\n",
       "158          grande\n",
       "16              dia\n",
       "316            jair\n",
       "488      presidente\n",
       "315          contra\n",
       "72              via\n",
       "468            pa√≠s\n",
       "533          sempre\n",
       "151           vamos\n",
       "439     bolsonarosp\n",
       "284        economia\n",
       "530        esquerda\n",
       "918             boa\n",
       "711         verdade\n",
       "22            forte\n",
       "162        parab√©ns\n",
       "1214         estado\n",
       "1348           anos\n",
       "208        ministro\n",
       "205            tudo\n",
       "703               √±\n",
       "92              bem\n",
       "15              bom\n",
       "Name: Palavra, dtype: object"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dic_jb = dict()\n",
    "for palavra in palavras_jb_sem_sw:\n",
    "    if palavra in dic_jb:\n",
    "        dic_jb[palavra] = dic_jb[palavra] + 1\n",
    "    else:\n",
    "        dic_jb[palavra] = 1\n",
    "        \n",
    "dic_jb = pd.DataFrame(dic_jb.items(), columns=['Palavra', 'Qtde'])\n",
    "\n",
    "# Eliminando caracteres que n√£o sairam com as stopwords\n",
    "dic_jb.drop([1, 3, 458], inplace=True)\n",
    "\n",
    "# Instaciando a variavel TOP 30 palavras\n",
    "nuvem_jb = dic_jb.sort_values(['Qtde'],ascending=False).head(30)\n",
    "\n",
    "nuvem_jb.Palavra"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4. Palavras mais citados nos Tweets do Lula**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.1. Removendo os StopWords**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "palavras_lu_sem_sw = []\n",
    "for palavra in palavras_lu:\n",
    "    if palavra in sw:\n",
    "        #print(palavra)\n",
    "        pass\n",
    "    else:\n",
    "        palavras_lu_sem_sw.append(palavra)\n",
    "#palavras_lu_sem_sw"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**4.4.2. Quantificando as palavras mais citadas**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'NoneType' object has no attribute 'head'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-14-fb9ddb25313e>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m     14\u001b[0m \u001b[1;31m# Instaciando a variavel TOP 30 palavras\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m---> 15\u001b[1;33m \u001b[0mnuvem_lu\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mdic_lu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0msort_values\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;34m'Qtde'\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m,\u001b[0m\u001b[0mascending\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mFalse\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0minplace\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;32mTrue\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mhead\u001b[0m\u001b[1;33m(\u001b[0m\u001b[1;36m30\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m     16\u001b[0m \u001b[0mnuvem_lu\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'NoneType' object has no attribute 'head'"
     ]
    }
   ],
   "source": [
    "dic_lu = dict()\n",
    "\n",
    "for palavra in palavras_lu_sem_sw:\n",
    "    if palavra in dic_lu:\n",
    "        dic_lu[palavra] = dic_lu[palavra] + 1\n",
    "    else:\n",
    "        dic_lu[palavra] = 1\n",
    "        \n",
    "dic_lu = pd.DataFrame(dic_lu.items(), columns=['Palavra', 'Qtde'])\n",
    "\n",
    "# Eliminando caracteres que n√£o sairam com as stopwords\n",
    "dic_lu.drop([12, 10, 63, 76], inplace=True)\n",
    "\n",
    "# Instaciando a variavel TOP 30 palavras\n",
    "nuvem_lu = dic_lu.sort_values(['Qtde'],ascending=False, inplace=True).head(30)\n",
    "nuvem_lu"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5. APRESENTA√á√ÉO DOS DADOS ATRAV√âS DO WORDCLOUD**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.1. Importando o M√≥dulo Matplotlib**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "ename": "ModuleNotFoundError",
     "evalue": "No module named 'wordcloud'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m                       Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-1-e8eb59de0ca9>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mmatplotlib\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpyplot\u001b[0m \u001b[1;32mas\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[1;32mfrom\u001b[0m \u001b[0mwordcloud\u001b[0m \u001b[1;32mimport\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[1;31mModuleNotFoundError\u001b[0m: No module named 'wordcloud'"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "from wordcloud import WordCloud"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.2. WordCloud Bolsonaro**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'palavra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-32-b5f54b7e6ab8>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      6\u001b[0m \u001b[0mnuvem_palavras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 7\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mpalavra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      8\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_count_lu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPalavra\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      9\u001b[0m      \u001b[0mnuvem_palavras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnuvem_palavras\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'palavra'"
     ]
    }
   ],
   "source": [
    "#nuvem_palavras = ' '\n",
    "#stopwords = set(sw.Palavra.values) \n",
    "#for words in df_count_jb.Palavra: \n",
    "     #nuvem_palavras = nuvem_palavras + words + ' '       \n",
    "    \n",
    "nuvem_palavras = ' '\n",
    "stopwords = set(sw.Palavra.values) \n",
    "for words in df_count_lu.Palavra: \n",
    "     nuvem_palavras = nuvem_palavras + words + ' '    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-90-572b6552c2ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the wordcloud object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnuvem_palavras\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Display the generated image:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfacecolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \"\"\"\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \"\"\"\n\u001b[0;32m    600\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[1;32m--> 391\u001b[1;33m                              \"got %d.\" % len(frequencies))\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=480, height=480, margin=0).generate(nuvem_jb)\n",
    " \n",
    "# Display the generated image:\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**5.3. WordCloud Lula**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'list' object has no attribute 'Palavra'",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-91-676f0ab5faa5>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[0mnuvem_palavras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mstopwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mset\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0msw\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPalavra\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mvalues\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;32mfor\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;32min\u001b[0m \u001b[0mdf_count_lu\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mPalavra\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m      \u001b[0mnuvem_palavras\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mnuvem_palavras\u001b[0m \u001b[1;33m+\u001b[0m \u001b[0mwords\u001b[0m \u001b[1;33m+\u001b[0m \u001b[1;34m' '\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mAttributeError\u001b[0m: 'list' object has no attribute 'Palavra'"
     ]
    }
   ],
   "source": [
    "nuvem_palavras = ' '\n",
    "stopwords = set(sw.Palavra.values) \n",
    "for words in df_count_lu.Palavra: \n",
    "     nuvem_palavras = nuvem_palavras + words + ' '       "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "ename": "ValueError",
     "evalue": "We need at least 1 word to plot a word cloud, got 0.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mValueError\u001b[0m                                Traceback (most recent call last)",
      "\u001b[1;32m<ipython-input-92-572b6552c2ab>\u001b[0m in \u001b[0;36m<module>\u001b[1;34m\u001b[0m\n\u001b[0;32m      1\u001b[0m \u001b[1;31m# Create the wordcloud object\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m----> 2\u001b[1;33m \u001b[0mwordcloud\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mWordCloud\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwidth\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mheight\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m480\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mmargin\u001b[0m\u001b[1;33m=\u001b[0m\u001b[1;36m0\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mnuvem_palavras\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m      3\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      4\u001b[0m \u001b[1;31m# Display the generated image:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m      5\u001b[0m \u001b[0mplt\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mfigure\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfigsize\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;33m(\u001b[0m\u001b[1;36m8\u001b[0m\u001b[1;33m,\u001b[0m \u001b[1;36m8\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m,\u001b[0m \u001b[0mfacecolor\u001b[0m \u001b[1;33m=\u001b[0m \u001b[1;32mNone\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    617\u001b[0m         \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    618\u001b[0m         \"\"\"\n\u001b[1;32m--> 619\u001b[1;33m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    620\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    621\u001b[0m     \u001b[1;32mdef\u001b[0m \u001b[0m_check_generated\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_text\u001b[1;34m(self, text)\u001b[0m\n\u001b[0;32m    599\u001b[0m         \"\"\"\n\u001b[0;32m    600\u001b[0m         \u001b[0mwords\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mprocess_text\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mtext\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[1;32m--> 601\u001b[1;33m         \u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mgenerate_from_frequencies\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mwords\u001b[0m\u001b[1;33m)\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0m\u001b[0;32m    602\u001b[0m         \u001b[1;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    603\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;32mc:\\users\\hp\\appdata\\local\\programs\\python\\python37\\lib\\site-packages\\wordcloud\\wordcloud.py\u001b[0m in \u001b[0;36mgenerate_from_frequencies\u001b[1;34m(self, frequencies, max_font_size)\u001b[0m\n\u001b[0;32m    389\u001b[0m         \u001b[1;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[1;33m(\u001b[0m\u001b[0mfrequencies\u001b[0m\u001b[1;33m)\u001b[0m \u001b[1;33m<=\u001b[0m \u001b[1;36m0\u001b[0m\u001b[1;33m:\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    390\u001b[0m             raise ValueError(\"We need at least 1 word to plot a word cloud, \"\n\u001b[1;32m--> 391\u001b[1;33m                              \"got %d.\" % len(frequencies))\n\u001b[0m\u001b[0;32m    392\u001b[0m         \u001b[0mfrequencies\u001b[0m \u001b[1;33m=\u001b[0m \u001b[0mfrequencies\u001b[0m\u001b[1;33m[\u001b[0m\u001b[1;33m:\u001b[0m\u001b[0mself\u001b[0m\u001b[1;33m.\u001b[0m\u001b[0mmax_words\u001b[0m\u001b[1;33m]\u001b[0m\u001b[1;33m\u001b[0m\u001b[1;33m\u001b[0m\u001b[0m\n\u001b[0;32m    393\u001b[0m \u001b[1;33m\u001b[0m\u001b[0m\n",
      "\u001b[1;31mValueError\u001b[0m: We need at least 1 word to plot a word cloud, got 0."
     ]
    }
   ],
   "source": [
    "# Create the wordcloud object\n",
    "wordcloud = WordCloud(width=480, height=480, margin=0).generate(nuvem_palavras)\n",
    " \n",
    "# Display the generated image:\n",
    "plt.figure(figsize = (8, 8), facecolor = None) \n",
    "plt.imshow(wordcloud) \n",
    "plt.axis(\"off\") \n",
    "plt.tight_layout(pad = 0) \n",
    "  \n",
    "plt.show() "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**6. CONCLUS√ÉO**\n",
    "\n",
    "Como conclus√£o da referida an√°lise √© poss√≠vel constatar uma coer√™ncia entre as palavras mais citadas por cada um dos pol√≠ticos e sua forte rela√ß√£o com as respectivas ideologias e cren√ßas que defendem, visando a cada postagem mais apoio de seus seguidores."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**7. BIBLIOGRAFIA**\n",
    "\n",
    "Twetter.Tweet objects. Dispon√≠evel em https://developer.twitter.com/en/docs/tweets/data-dictionary/overview/tweet-object\n",
    "\n",
    "Join Extra Crunch. Twitter Adds ‚ÄúPossibly Sensitive‚Äù Designation To Tweets With NSFW Content, 28 julho 2011. Dispon√≠vel em https://techcrunch.com/2011/07/28/twitter-adds-possibly-sensitive-designation-to-tweets-with-nsfw-content/ > Acesso em 28 jan 2020."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
